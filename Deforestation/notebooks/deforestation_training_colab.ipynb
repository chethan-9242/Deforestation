{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deforestation Detection - Land Cover Segmentation\n",
        "\n",
        "This notebook trains a U-Net model for land cover classification using satellite imagery.\n",
        "\n",
        "## Dataset\n",
        "- **Training**: 60 image-mask pairs\n",
        "- **Validation**: 14 image-mask pairs\n",
        "- **Classes**: 5 (urban, water, forest, agriculture, road)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install albumentations\n",
        "!pip install tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Segmentation models\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload your dataset to Colab\n",
        "# Method 1: Upload zip file and extract\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "# Upload the dataset zip file\n",
        "print(\"Please upload your dataset zip file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the zip file\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('dataset')\n",
        "        print(f\"Extracted {filename} to dataset/ folder\")\n",
        "\n",
        "# Method 2: If you have the dataset in Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# dataset_path = '/content/drive/MyDrive/your_dataset_path'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset configuration\n",
        "class_dict = {\n",
        "    'urban': (0, 255, 255),\n",
        "    'water': (0, 0, 255), \n",
        "    'forest': (0, 255, 0),\n",
        "    'agriculture': (255, 255, 0),\n",
        "    'road': (255, 0, 255)\n",
        "}\n",
        "\n",
        "num_classes = len(class_dict)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(\"Class mapping:\", class_dict)\n",
        "\n",
        "# Dataset paths (adjust based on your upload method)\n",
        "train_image_dir = 'dataset/train_image'\n",
        "train_mask_dir = 'dataset/train_mask'\n",
        "val_image_dir = 'dataset/val_image'\n",
        "val_mask_dir = 'dataset/val_mask'\n",
        "\n",
        "# Check if paths exist\n",
        "print(\"Checking dataset paths...\")\n",
        "print(f\"Train images exist: {os.path.exists(train_image_dir)}\")\n",
        "print(f\"Train masks exist: {os.path.exists(train_mask_dir)}\")\n",
        "print(f\"Val images exist: {os.path.exists(val_image_dir)}\")\n",
        "print(f\"Val masks exist: {os.path.exists(val_mask_dir)}\")\n",
        "\n",
        "# If validation doesn't exist, use test data or split training data\n",
        "if not os.path.exists(val_image_dir) or not os.path.exists(val_mask_dir):\n",
        "    print(\"Validation data not found. Using test data or splitting training data...\")\n",
        "    # Try test data first\n",
        "    test_image_dir = 'dataset/test_image'\n",
        "    test_mask_dir = 'dataset/test_mask'\n",
        "    \n",
        "    if os.path.exists(test_image_dir) and os.path.exists(test_mask_dir):\n",
        "        val_image_dir = test_image_dir\n",
        "        val_mask_dir = test_mask_dir\n",
        "        print(\"Using test data for validation\")\n",
        "    else:\n",
        "        # Split training data\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        print(\"Splitting training data for validation...\")\n",
        "        # This will be handled in the dataset creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Dataset Class\n",
        "class LandCoverDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, num_classes=5, split_ratio=None):\n",
        "        self.image_dir = Path(image_dir)\n",
        "        self.mask_dir = Path(mask_dir)\n",
        "        self.transform = transform\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Get all image files\n",
        "        self.image_files = list(self.image_dir.glob('*.jpg'))\n",
        "        self.mask_files = list(self.mask_dir.glob('*.png'))\n",
        "        \n",
        "        # Filter for matching pairs\n",
        "        self.valid_pairs = []\n",
        "        for img_file in self.image_files:\n",
        "            mask_file = self.mask_dir / f\"{img_file.stem}.png\"\n",
        "            if mask_file.exists():\n",
        "                self.valid_pairs.append((img_file, mask_file))\n",
        "        \n",
        "        # If split_ratio is provided, split the data\n",
        "        if split_ratio is not None:\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            train_pairs, val_pairs = train_test_split(\n",
        "                self.valid_pairs, \n",
        "                test_size=split_ratio, \n",
        "                random_state=42\n",
        "            )\n",
        "            self.valid_pairs = train_pairs if split_ratio < 0.5 else val_pairs\n",
        "        \n",
        "        print(f\"Found {len(self.valid_pairs)} valid image-mask pairs\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.valid_pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.valid_pairs[idx]\n",
        "        \n",
        "        # Load image and mask\n",
        "        image = np.array(Image.open(img_path).convert('RGB'))\n",
        "        mask = np.array(Image.open(mask_path).convert('RGB'))\n",
        "        \n",
        "        # Convert mask to class indices\n",
        "        mask_indices = self.rgb_to_class_indices(mask)\n",
        "        \n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask_indices)\n",
        "            image = augmented['image']\n",
        "            mask_indices = augmented['mask']\n",
        "        \n",
        "        return image, mask_indices.long()\n",
        "    \n",
        "    def rgb_to_class_indices(self, mask_rgb):\n",
        "        \"\"\"Convert RGB mask to class indices\"\"\"\n",
        "        mask_indices = np.zeros((mask_rgb.shape[0], mask_rgb.shape[1]), dtype=np.uint8)\n",
        "        \n",
        "        for i, (class_name, rgb_color) in enumerate(class_dict.items()):\n",
        "            # Find pixels matching this class color\n",
        "            class_mask = np.all(mask_rgb == rgb_color, axis=2)\n",
        "            mask_indices[class_mask] = i\n",
        "        \n",
        "        return mask_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data augmentation and transforms\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "# Check if validation data exists, if not split training data\n",
        "if os.path.exists(val_image_dir) and os.path.exists(val_mask_dir):\n",
        "    print(\"Using separate validation data\")\n",
        "    train_dataset = LandCoverDataset(train_image_dir, train_mask_dir, transform=train_transform)\n",
        "    val_dataset = LandCoverDataset(val_image_dir, val_mask_dir, transform=val_transform)\n",
        "else:\n",
        "    print(\"Splitting training data for validation (80% train, 20% val)\")\n",
        "    # Create full dataset first\n",
        "    full_dataset = LandCoverDataset(train_image_dir, train_mask_dir, transform=None)\n",
        "    \n",
        "    # Split into train and val\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    train_pairs, val_pairs = train_test_split(full_dataset.valid_pairs, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Create datasets with split data\n",
        "    train_dataset = LandCoverDataset(train_image_dir, train_mask_dir, transform=train_transform)\n",
        "    train_dataset.valid_pairs = train_pairs\n",
        "    \n",
        "    val_dataset = LandCoverDataset(train_image_dir, train_mask_dir, transform=val_transform)\n",
        "    val_dataset.valid_pairs = val_pairs\n",
        "    \n",
        "    print(f\"Training samples: {len(train_dataset.valid_pairs)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset.valid_pairs)}\")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "# Verify data loaders\n",
        "print(f\"Train loader batches: {len(train_loader)}\")\n",
        "print(f\"Val loader batches: {len(val_loader)}\")\n",
        "\n",
        "# Test loading a batch\n",
        "if len(val_loader) > 0:\n",
        "    print(\"Validation data loaded successfully!\")\n",
        "else:\n",
        "    print(\"ERROR: No validation data found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create U-Net model\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",        # Use ResNet34 as backbone\n",
        "    encoder_weights=\"imagenet\",     # Use ImageNet pretrained weights\n",
        "    in_channels=3,                  # RGB input\n",
        "    classes=num_classes,            # 5 classes\n",
        "    activation=None,                # No activation (we'll use CrossEntropyLoss)\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_pixels = 0\n",
        "    total_pixels = 0\n",
        "    \n",
        "    for images, masks in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        # Calculate pixel accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_pixels += (predicted == masks).sum().item()\n",
        "        total_pixels += masks.numel()\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct_pixels / total_pixels\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Validation function\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_pixels = 0\n",
        "    total_pixels = 0\n",
        "    \n",
        "    # Check if validation loader is empty\n",
        "    if len(val_loader) == 0:\n",
        "        print(\"Warning: Validation loader is empty!\")\n",
        "        return 0.0, 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            # Calculate pixel accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_pixels += (predicted == masks).sum().item()\n",
        "            total_pixels += masks.numel()\n",
        "    \n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = correct_pixels / total_pixels if total_pixels > 0 else 0.0\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "num_epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Number of epochs: {num_epochs}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Training\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    # Print metrics\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(\"New best model saved!\")\n",
        "    \n",
        "    # Early stopping (optional)\n",
        "    if epoch > 10 and val_loss > best_val_loss * 1.1:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "print(\"\\nTraining completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Train Accuracy')\n",
        "plt.plot(val_accs, label='Val Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Final training accuracy: {train_accs[-1]:.4f}\")\n",
        "print(f\"Final validation accuracy: {val_accs[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model and make predictions\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Visualize predictions\n",
        "def visualize_predictions(model, val_loader, device, num_samples=4):\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (images, masks) in enumerate(val_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "                \n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            \n",
        "            # Convert to numpy for visualization\n",
        "            img = images[0].cpu().permute(1, 2, 0).numpy()\n",
        "            img = (img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
        "            img = np.clip(img, 0, 1)\n",
        "            \n",
        "            true_mask = masks[0].cpu().numpy()\n",
        "            pred_mask = predictions[0].cpu().numpy()\n",
        "            \n",
        "            # Plot original image\n",
        "            axes[i, 0].imshow(img)\n",
        "            axes[i, 0].set_title('Original Image')\n",
        "            axes[i, 0].axis('off')\n",
        "            \n",
        "            # Plot true mask\n",
        "            axes[i, 1].imshow(true_mask, cmap='tab10')\n",
        "            axes[i, 1].set_title('True Mask')\n",
        "            axes[i, 1].axis('off')\n",
        "            \n",
        "            # Plot predicted mask\n",
        "            axes[i, 2].imshow(pred_mask, cmap='tab10')\n",
        "            axes[i, 2].set_title('Predicted Mask')\n",
        "            axes[i, 2].axis('off')\n",
        "            \n",
        "            # Plot difference\n",
        "            diff = (true_mask != pred_mask).astype(float)\n",
        "            axes[i, 3].imshow(diff, cmap='Reds')\n",
        "            axes[i, 3].set_title('Difference (Red = Wrong)')\n",
        "            axes[i, 3].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show predictions\n",
        "visualize_predictions(model, val_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the trained model\n",
        "from google.colab import files\n",
        "\n",
        "# Save the model\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'class_dict': class_dict,\n",
        "    'num_classes': num_classes,\n",
        "    'model_config': {\n",
        "        'encoder_name': 'resnet34',\n",
        "        'encoder_weights': 'imagenet',\n",
        "        'in_channels': 3,\n",
        "        'classes': num_classes\n",
        "    }\n",
        "}, 'deforestation_model.pth')\n",
        "\n",
        "# Download the model\n",
        "files.download('deforestation_model.pth')\n",
        "print(\"Model downloaded successfully!\")\n",
        "\n",
        "# Also download the training curves data\n",
        "import json\n",
        "training_data = {\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'train_accs': train_accs,\n",
        "    'val_accs': val_accs,\n",
        "    'best_val_loss': best_val_loss\n",
        "}\n",
        "\n",
        "with open('training_curves.json', 'w') as f:\n",
        "    json.dump(training_data, f)\n",
        "\n",
        "files.download('training_curves.json')\n",
        "print(\"Training curves data downloaded!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
